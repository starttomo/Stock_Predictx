# model/train.py - æ”¹è¿›ç‰ˆæœ¬ï¼ˆå«å¯è§†åŒ–ï¼‰
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from data.loader import get_hs300_data
from model.lstm_model import ImprovedLSTMModel
from database.models import db, Forecast
from datetime import datetime, timedelta
import joblib
import os
import matplotlib.pyplot as plt

# å¯¼å…¥å¯è§†åŒ–æ¨¡å—
from model.visualization import ModelVisualizer, create_training_plots, create_evaluation_plots, create_forecast_plots


def create_features(df):
    """åˆ›å»ºæ›´å¤šæŠ€æœ¯æŒ‡æ ‡ä½œä¸ºç‰¹å¾"""
    close = df['close']

    # ä»·æ ¼ç‰¹å¾
    df['returns'] = close.pct_change()
    df['log_returns'] = np.log(close / close.shift(1))

    # ç§»åŠ¨å¹³å‡çº¿
    for window in [5, 10, 20, 30, 60]:
        df[f'ma{window}'] = close.rolling(window).mean()
        df[f'ma_ratio_{window}'] = close / df[f'ma{window}']

    # æ³¢åŠ¨ç‡
    df['volatility_5'] = df['returns'].rolling(5).std()
    df['volatility_20'] = df['returns'].rolling(20).std()

    # åŠ¨é‡æŒ‡æ ‡
    df['momentum_5'] = close / close.shift(5) - 1
    df['momentum_10'] = close / close.shift(10) - 1

    # å¸ƒæ—å¸¦
    df['bb_middle'] = close.rolling(20).mean()
    bb_std = close.rolling(20).std()
    df['bb_upper'] = df['bb_middle'] + 2 * bb_std
    df['bb_lower'] = df['bb_middle'] - 2 * bb_std
    df['bb_position'] = (close - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

    # RSI
    delta = close.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))

    # MACD
    exp1 = close.ewm(span=12).mean()
    exp2 = close.ewm(span=26).mean()
    df['macd'] = exp1 - exp2
    df['macd_signal'] = df['macd'].ewm(span=9).mean()

    # æˆäº¤é‡ç‰¹å¾
    df['volume_ma'] = df['volume'].rolling(20).mean()
    df['volume_ratio'] = df['volume'] / df['volume_ma']

    # ä»·æ ¼ä½ç½®ç‰¹å¾
    df['high_52'] = close.rolling(252).max()
    df['low_52'] = close.rolling(252).min()
    df['price_position'] = (close - df['low_52']) / (df['high_52'] - df['low_52'])

    return df.dropna()


def prepare_sequences_multivariate(df, seq_len, target_col='close'):
    """å‡†å¤‡å¤šå˜é‡åºåˆ—æ•°æ®ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨ç‰¹å¾"""
    # åŠ¨æ€è·å–ç‰¹å¾åˆ—ï¼ˆé™¤ 'date'ï¼‰
    feature_cols = [col for col in df.columns if col != 'date']
    features = df[feature_cols].values
    targets = df[target_col].values  # æ³¨æ„ï¼šå¦‚æœ df å·²ç¼©æ”¾ï¼Œtargets ä¹Ÿæ˜¯ç¼©æ”¾åçš„

    X, y = [], []
    for i in range(len(features) - seq_len):
        X.append(features[i:i + seq_len])
        y.append(targets[i + seq_len])

    return np.array(X), np.array(y)


def train_and_forecast_improved():
    print("=" * 60)
    print("å¼€å§‹æ”¹è¿›ç‰ˆæ¨¡å‹è®­ç»ƒ...")
    print("=" * 60)

    # åˆå§‹åŒ–å¯è§†åŒ–å™¨
    visualizer = ModelVisualizer(save_dir='model/plots')

    # è·å–æ•°æ®å¹¶åˆ›å»ºç‰¹å¾
    print("Step 1: åŠ è½½æ•°æ®å¹¶åˆ›å»ºç‰¹å¾...")
    df = get_hs300_data()
    df = create_features(df)
    feature_cols = [col for col in df.columns if col != 'date']
    print(f"âœ“ ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"âœ“ ç‰¹å¾åˆ—è¡¨: {', '.join(feature_cols[:10])}{'...' if len(feature_cols) > 10 else ''}")

    # æ•°æ®æ ‡å‡†åŒ–ï¼ˆå…ˆç¼©æ”¾æ‰€æœ‰ç‰¹å¾ï¼ŒåŒ…æ‹¬ closeï¼‰
    print("\nStep 2: æ•°æ®æ ‡å‡†åŒ–...")
    scaler_X = MinMaxScaler()
    data_scaled = scaler_X.fit_transform(df[feature_cols])
    scaled_df = pd.DataFrame(data_scaled, columns=feature_cols, index=df.index)
    scaled_df['date'] = df['date']

    # å•ç‹¬çš„ scaler_y åªç”¨äº closeï¼ˆä¾¿äºåç¼©æ”¾é¢„æµ‹ï¼‰
    scaler_y = MinMaxScaler()
    scaler_y.fit(df[['close']])  # åª fit åŸå§‹ close
    print("âœ“ æ ‡å‡†åŒ–å®Œæˆ")

    # å‡†å¤‡åºåˆ—æ•°æ®ï¼ˆä½¿ç”¨ç¼©æ”¾åçš„ dfï¼‰
    print("\nStep 3: å‡†å¤‡åºåˆ—æ•°æ®...")
    seq_len = 60
    X, y = prepare_sequences_multivariate(scaled_df, seq_len)

    if len(X) == 0:
        print("é”™è¯¯ï¼šæ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆåºåˆ—")
        return

    print(f"âœ“ æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")

    # åˆ†å‰²æ•°æ®
    split = int(0.8 * len(X))
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]

    # åˆ›å»ºæ”¹è¿›çš„LSTMæ¨¡å‹
    print("\nStep 4: åˆ›å»ºæ¨¡å‹...")
    input_size = X.shape[2]  # ç°åœ¨æ˜¯æ‰€æœ‰ç‰¹å¾çš„æ•°é‡
    model = ImprovedLSTMModel(input_size=input_size, hidden_size=100, num_layers=3, output_size=1)
    print(f"âœ“ æ¨¡å‹ç»“æ„: {model}")

    # è®­ç»ƒé…ç½®
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    # è®°å½•è®­ç»ƒå†å²
    train_losses = []
    val_losses = []
    lrs = []
    best_loss = float('inf')
    patience = 40
    patience_counter = 0

    print("\nStep 5: å¼€å§‹è®­ç»ƒ...")
    print("=" * 40)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(200):
        model.train()
        train_loss = 0
        num_batches = 0

        for i in range(0, len(X_train), 32):
            batch_x = torch.FloatTensor(X_train[i:i + 32])
            batch_y = torch.FloatTensor(y_train[i:i + 32].reshape(-1, 1))

            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            train_loss += loss.item()
            num_batches += 1

        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_pred = model(torch.FloatTensor(X_test))
            val_loss = criterion(val_pred, torch.FloatTensor(y_test.reshape(-1, 1)))

        # è®°å½•å†å²
        train_losses.append(train_loss / num_batches)
        val_losses.append(val_loss.item())
        lrs.append(optimizer.param_groups[0]['lr'])

        scheduler.step(val_loss)

        if epoch % 20 == 0:
            print(f"Epoch {epoch:3d} | Train Loss: {train_loss / num_batches:.6f} | "
                  f"Val Loss: {val_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}")

        # æ—©åœ
        if val_loss < best_loss:
            best_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'model/best_model.pth')
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"â†’ æ—©åœäºç¬¬ {epoch} è½®")
            break

    print("=" * 40)
    print(f"âœ“ è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")

    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load('model/best_model.pth'))

    # ä¿å­˜scaler
    os.makedirs('model', exist_ok=True)
    joblib.dump(scaler_X, 'model/scaler_X.pkl')
    joblib.dump(scaler_y, 'model/scaler_y.pkl')
    print("âœ“ æ¨¡å‹å’ŒScalerå·²ä¿å­˜")

    # === å¯è§†åŒ–éƒ¨åˆ† ===
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("=" * 60)

    # 1. è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
    print("1. ç”Ÿæˆè®­ç»ƒå†å²å›¾è¡¨...")
    visualizer.plot_training_history(
        train_losses, val_losses, lrs,
        save_path='model/plots/training_history.png',
        show=False
    )

    # 2. æ¨¡å‹è¯„ä¼°å¯è§†åŒ–
    print("2. ç”Ÿæˆæ¨¡å‹è¯„ä¼°å›¾è¡¨...")
    model.eval()
    with torch.no_grad():
        test_predictions_scaled = model(torch.FloatTensor(X_test)).numpy()

    test_predictions = scaler_y.inverse_transform(test_predictions_scaled)
    y_test_true = scaler_y.inverse_transform(y_test.reshape(-1, 1))
    test_dates = df['date'].iloc[split + seq_len:].values

    # è¯„ä¼°æŒ‡æ ‡
    from sklearn.metrics import mean_absolute_error, mean_squared_error
    mae = mean_absolute_error(y_test_true, test_predictions)
    rmse = np.sqrt(mean_squared_error(y_test_true, test_predictions))

    print(f"   - MAE: {mae:.2f}")
    print(f"   - RMSE: {rmse:.2f}")

    # ç”Ÿæˆè¯„ä¼°å›¾è¡¨
    visualizer.plot_predictions_vs_actual(
        y_test_true, test_predictions, test_dates,
        save_path='model/plots/evaluation.png',
        show=False
    )

    visualizer.plot_direction_accuracy(
        y_test_true, test_predictions, test_dates,
        save_path='model/plots/direction_accuracy.png',
        show=False
    )

    # 3. ç‰¹å¾ç›¸å…³æ€§å¯è§†åŒ–
    print("3. ç”Ÿæˆç‰¹å¾ç›¸å…³æ€§å›¾è¡¨...")
    visualizer.plot_feature_correlation(
        df, feature_cols,
        save_path='model/plots/feature_correlation.png',
        show=False
    )

    # 4. é¢„æµ‹æœªæ¥å¹¶å¯è§†åŒ–
    print("\nStep 6: é¢„æµ‹æœªæ¥5æ—¥ä»·æ ¼...")
    model.eval()
    last_sequence = X[-1:].copy()

    future_dates = []
    future_preds = []
    current_date = df['date'].iloc[-1]

    with torch.no_grad():
        temp_sequence = last_sequence.copy()
        print("\né¢„æµ‹è¿›åº¦:")

        for i in range(5):
            next_date = current_date + timedelta(days=1)
            while next_date.weekday() >= 5:  # è·³è¿‡å‘¨æœ«
                next_date += timedelta(days=1)

            # é¢„æµ‹
            pred_scaled = model(torch.FloatTensor(temp_sequence)).numpy()[0, 0]
            pred_price = scaler_y.inverse_transform([[pred_scaled]])[0, 0]

            future_preds.append(pred_price)
            future_dates.append(next_date)

            # æ›´æ–°åºåˆ—
            new_row = temp_sequence[0, -1, :].copy()
            new_row[0] = pred_scaled
            temp_sequence = np.roll(temp_sequence, -1, axis=1)
            temp_sequence[0, -1, :] = new_row

            current_date = next_date
            print(f"   - {next_date.strftime('%Y-%m-%d')}: {pred_price:.2f}")

    # ç”Ÿæˆé¢„æµ‹æ•ˆæœå›¾
    print("\n4. ç”Ÿæˆæœªæ¥é¢„æµ‹å›¾è¡¨...")
    historical_dates = df['date'].iloc[-60:].values  # æœ€è¿‘60å¤©
    historical_prices = df['close'].iloc[-60:].values

    forecast_lower = [p - p * 0.03 for p in future_preds]
    forecast_upper = [p + p * 0.03 for p in future_preds]

    visualizer.plot_future_forecast(
        historical_dates, historical_prices,
        future_dates, future_preds,
        forecast_lower, forecast_upper,
        save_path='model/plots/future_forecast.png',
        show=False
    )

    # å­˜å…¥æ•°æ®åº“
    print("\nStep 7: ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ•°æ®åº“...")
    for date, pred in zip(future_dates, future_preds):
        uncertainty = pred * 0.03
        f = Forecast.query.get(date.date())
        if not f:
            f = Forecast(
                date=date.date(),
                yhat=pred,
                yhat_lower=pred - uncertainty,
                yhat_upper=pred + uncertainty
            )
            db.session.add(f)
        else:
            f.yhat = pred
            f.yhat_lower = pred - uncertainty
            f.yhat_upper = pred + uncertainty

    db.session.commit()

    print("\n" + "=" * 60)
    print("æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼å›¾è¡¨å·²ä¿å­˜è‡³ model/plots/ ç›®å½•")
    print("=" * 60)

    # æ‰“å°æ€»ç»“æŠ¥å‘Š
    print("\nğŸ“Š è®­ç»ƒæ€»ç»“æŠ¥å‘Š:")
    print(f"   - è®­ç»ƒè½®æ¬¡: {len(train_losses)}")
    print(f"   - æœ€ä½³éªŒè¯æŸå¤±: {best_loss:.6f}")
    print(f"   - æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
    print(f"   - æµ‹è¯•é›† MAE: {mae:.2f}")
    print(f"   - æµ‹è¯•é›† RMSE: {rmse:.2f}")
    print(f"\n   - å›¾è¡¨ä½ç½®:")
    print(f"     * è®­ç»ƒå†å²: model/plots/training_history.png")
    print(f"     * è¯„ä¼°ç»“æœ: model/plots/evaluation.png")
    print(f"     * æ–¹å‘å‡†ç¡®ç‡: model/plots/direction_accuracy.png")
    print(f"     * ç‰¹å¾ç›¸å…³æ€§: model/plots/feature_correlation.png")
    print(f"     * æœªæ¥é¢„æµ‹: model/plots/future_forecast.png")


if __name__ == "__main__":
    from app import app

    with app.app_context():
        train_and_forecast_improved()